{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpustY83b7zwY+k2oNoYG0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalejan/EEG-Neural-Representation-Prediciton-Model_BA/blob/main/EEG_Preprocessing_allsubs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1. DOWNLOADING THE WHOLE BATCH OF EEG SIGNALS FOR ALL SUBJECTS"
      ],
      "metadata": {
        "id": "Cp-phkNk-Sx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openneuro-py"
      ],
      "metadata": {
        "collapsed": true,
        "id": "LeuPshBY_KnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx0BSfxX8JJy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openneuro"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zhTzFl9b-mZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_dir = \"/content/drive/MyDrive/EEG_data/full_subject_list\"\n"
      ],
      "metadata": {
        "id": "5z-LlXOi-usf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)"
      ],
      "metadata": {
        "id": "_vcx0-QV_XZl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openneuro.download(dataset='ds004771', target_dir=target_dir)"
      ],
      "metadata": {
        "id": "5T-BfLcQ_ak9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 2. LOADING THE EEG DATA INTO PY"
      ],
      "metadata": {
        "id": "Z3bk1bw3BBl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mne"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GyyMFMQaBQAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mne\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import glob"
      ],
      "metadata": {
        "id": "lvzs_eZrBJRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_path = '/content/drive/MyDrive/EEG_data/full_subject_list'\n",
        "save_path = '/content/drive/MyDrive/EEG_data/P600_Results/all_subs'"
      ],
      "metadata": {
        "id": "0QPiu0-GBL_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)"
      ],
      "metadata": {
        "id": "xjhbew9zBhfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysis Parameters\n",
        "roi_channels = ['Pz', 'Cz', 'CP1', 'CP2', 'P3', 'P4']\n",
        "t_min, t_max = 0.5, 0.8  # 500-800ms\n",
        "l_freq, h_freq = 0.1, 30.0"
      ],
      "metadata": {
        "id": "4cUqFp0oBjJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3. LOADING PARTICIPANT LIST\n"
      ],
      "metadata": {
        "id": "WcnV_e7jDD0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsv_file = os.path.join(base_path, 'participants.tsv')"
      ],
      "metadata": {
        "id": "qmtKaWdABqKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(tsv_file):\n",
        "    meta_df = pd.read_csv(tsv_file, sep='\\t')\n",
        "\n",
        "    # FILTERING LOGIC: Keep only 'good' subjects\n",
        "    good_subjects = meta_df[meta_df['data_usability'] == 'good']\n",
        "    subject_list = good_subjects['participant_id'].tolist()\n",
        "\n",
        "    print(f\"üìã Metadata loaded.\")\n",
        "    print(f\"   - Total in file: {len(meta_df)}\")\n",
        "    print(f\"   - Excluded (Bad): {len(meta_df) - len(subject_list)}\")\n",
        "    print(f\"   - Included (Good): {len(subject_list)}\")\n",
        "\n",
        "else:\n",
        "    # Fallback if file is missing (process everything)\n",
        "    print(\"‚ö†Ô∏è participants.tsv not found. Processing ALL folders.\")\n",
        "    subject_list = [d for d in os.listdir(base_path) if d.startswith('sub-')]\n",
        "\n",
        "subject_list.sort()\n",
        "print(f\"‚úÖ Ready to process {len(subject_list)} subjects.\")"
      ],
      "metadata": {
        "id": "6Fhsyh9BBsRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4. PREPROCESSING LOOP\n",
        "\n",
        "Analysis Parameters\n",
        "<br>roi_channels = ['Pz', 'Cz', 'CP1', 'CP2', 'P3', 'P4'] <br>t_min, t_max = 0.5, 0.8  # 500-800ms  <br>l_freq, h_freq = 0.1, 30.0 <br>\n",
        "\n",
        "Below can be seen a batch-processing pipeline that iterates through our validated subject list. For each participant, it effectively 'cleans' the signal (filtering & baselining), translates the raw event codes into 'Control/Violation' conditions, and extracts the mean amplitude specifically from the P600 time window (500-800ms) over our parietal ROI. The system includes error handling to skip corrupted files without stopping the batch, and it outputs a master dataset ready for T-tests."
      ],
      "metadata": {
        "id": "gc5yMoGxCGOs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CHECK UP whether all participants are loaded"
      ],
      "metadata": {
        "id": "klVgqqf0GWIh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define Path (Check capitalization carefully!)\n",
        "base_path = '/content/drive/MyDrive/EEG_data/full_subject_list'\n",
        "tsv_path = os.path.join(base_path, 'participants.tsv')\n",
        "\n",
        "print(f\"üìÇ Checking Path: {base_path}\")"
      ],
      "metadata": {
        "id": "CsMlXwZ8FDxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Check if Metadata exists\n",
        "if os.path.exists(tsv_path):\n",
        "    print(\"‚úÖ participants.tsv FOUND.\")\n",
        "    df = pd.read_csv(tsv_path, sep='\\t')\n",
        "\n",
        "    # Check for the column name (handling capital letters/spaces)\n",
        "    col_name = [c for c in df.columns if 'usability' in c]\n",
        "    if col_name:\n",
        "        col = col_name[0]\n",
        "        good_subjects = df[df[col] == 'good']['participant_id'].tolist()\n",
        "        print(f\"   - The file lists {len(good_subjects)} 'GOOD' subjects (Target).\")\n",
        "        print(f\"   - The file lists {len(df) - len(good_subjects)} 'BAD' subjects.\")\n",
        "    else:\n",
        "        print(\"   ‚ùå Column 'data_usability' not found in TSV.\")\n",
        "else:\n",
        "    print(\"‚ùå CRITICAL: participants.tsv NOT found.\")\n",
        "    print(\"   (This is why the code processed everyone!)\")\n",
        "    # Check if it is inside a subfolder (common OpenNeuro issue)\n",
        "    alt_path = os.path.join(base_path, 'ds004771', 'participants.tsv')\n",
        "    if os.path.exists(alt_path):\n",
        "        print(f\"   üí° Found it in subfolder: {alt_path}\")\n",
        "        print(\"   -> We need to adjust your base_path.\")"
      ],
      "metadata": {
        "id": "JUzIQqK1Gx2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Check Actual Folders\n",
        "if os.path.exists(base_path):\n",
        "    folders = [f for f in os.listdir(base_path) if f.startswith('sub-') and os.path.isdir(os.path.join(base_path, f))]\n",
        "    folders.sort()\n",
        "    print(f\"\\nüìÅ Actual Folders on Drive: {len(folders)}\")\n",
        "\n",
        "    # Compare\n",
        "    if 'good_subjects' in locals():\n",
        "        missing = [s for s in good_subjects if s not in folders]\n",
        "        if missing:\n",
        "            print(f\"‚ö†Ô∏è MISSING DATA: You are missing these {len(missing)} 'GOOD' subjects:\")\n",
        "            print(f\"   {missing}\")\n",
        "        else:\n",
        "            print(\"‚úÖ All 45 Good subjects are present.\")"
      ],
      "metadata": {
        "id": "jRg1A4fhHAjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROCESSING ALL AT ONCE (ALL SUBS)"
      ],
      "metadata": {
        "id": "uHhV9kNqH7uZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_subjects_stats = []\n",
        "for sub_id in subject_list:\n",
        "    print(f\"\\nProcessing: {sub_id} ...\")\n",
        "\n",
        "    try:\n",
        "        # 1. Locate File\n",
        "        sub_folder = os.path.join(base_path, sub_id)\n",
        "        # Look for .set file recursively\n",
        "        set_files = glob.glob(os.path.join(sub_folder, '**', '*.set'), recursive=True)\n",
        "\n",
        "        if not set_files:\n",
        "            print(f\"   ‚ùå Skipped: No .set file found.\")\n",
        "            continue\n",
        "\n",
        "        fname = set_files[0]\n",
        "\n",
        "        # 2. Load & Preprocess\n",
        "        epochs = mne.read_epochs_eeglab(fname, verbose=False)\n",
        "        epochs.filter(l_freq, h_freq, verbose=False)\n",
        "        epochs.apply_baseline((-0.1, 0), verbose=False)\n",
        "\n",
        "        # 3. Resolve Labels\n",
        "        id_to_text = {v: k for k, v in epochs.event_id.items()}\n",
        "\n",
        "        def solve_label(event_id):\n",
        "            text_label = id_to_text.get(event_id, \"\")\n",
        "            if '(110)' in text_label or '(115)' in text_label: return 'Control'\n",
        "            if '(220)' in text_label or '(225)' in text_label: return 'Violation'\n",
        "            return None\n",
        "\n",
        "        # 4. Extract Data (P600)\n",
        "        available_picks = [c for c in roi_channels if c in epochs.ch_names]\n",
        "\n",
        "        data = epochs.get_data(tmin=t_min, tmax=t_max, picks=available_picks)\n",
        "        mean_vals = data.mean(axis=(1, 2)) * 1e6 # Convert to ¬µV\n",
        "\n",
        "        # 5. Build DataFrame\n",
        "        df = pd.DataFrame({\n",
        "            'subject_id': sub_id,\n",
        "            'mean_amplitude': mean_vals,\n",
        "            'condition_id': epochs.events[:, -1]\n",
        "        })\n",
        "\n",
        "        df['condition'] = df['condition_id'].apply(solve_label)\n",
        "        df_clean = df.dropna(subset=['condition'])\n",
        "\n",
        "        # 6. Save & Aggregate\n",
        "        df_clean.to_csv(os.path.join(save_path, f\"{sub_id}_results.csv\"), index=False)\n",
        "\n",
        "        # Summary for Grand Average\n",
        "        subj_summary = df_clean.groupby(['subject_id', 'condition'])['mean_amplitude'].mean().reset_index()\n",
        "        all_subjects_stats.append(subj_summary)\n",
        "\n",
        "        print(f\"   ‚úÖ Success.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå ERROR on {sub_id}: {str(e)}\")\n",
        "        continue"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ODIC1yixCDho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SAVING THE MASTER FILE (MASTER_Grand_Average.csv)\n",
        "\n",
        ".CSV serves for a Summary of all Statistics\n",
        "It collapses the brain waves into a single number (Mean Amplitude). Great for Statistics."
      ],
      "metadata": {
        "id": "uSnkb6KnME3b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# STEP C: SAVE MASTER FILE\n",
        "# ==========================================\n",
        "if all_subjects_stats:\n",
        "    master_df = pd.concat(all_subjects_stats, ignore_index=True)\n",
        "    master_path = os.path.join(save_path, 'MASTER_Grand_Average.csv')\n",
        "    master_df.to_csv(master_path, index=False)\n",
        "\n",
        "    print(\"\\n\" + \"=\"*30)\n",
        "    print(\"üéâ BATCH PROCESSING FINISHED\")\n",
        "    print(f\"üìä Master file saved to: {master_path}\")\n",
        "    print(master_df.head())\n",
        "else:\n",
        "    print(\"\\n‚ùå No data processed.\")"
      ],
      "metadata": {
        "id": "upBBve0WKZMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FINAL LOOP TO SAVE .FIF DATA"
      ],
      "metadata": {
        "id": "2wjQErWEMXSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING AND CONFIGURATION\n",
        "\n",
        "base_path = '/content/drive/MyDrive/EEG_data/full_subject_list'\n",
        "save_path = '/content/drive/MyDrive/EEG_data/P600_Results/all_subs'"
      ],
      "metadata": {
        "id": "iJdsseYaM6nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)"
      ],
      "metadata": {
        "id": "5kAjBbr_NYfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LOADING AND VERIFYING SUBJECT LIST\n",
        "\n",
        "# 1. Load the metadata file\n",
        "tsv_file = os.path.join(base_path, 'participants.tsv')\n",
        "meta_df = pd.read_csv(tsv_file, sep='\\t')\n",
        "\n",
        "# 2. Filter: Keep only \"good\" subjects\n",
        "# We explicitly check the 'data_usability' column\n",
        "good_subjects = meta_df[meta_df['data_usability'] == 'good']['participant_id'].tolist()\n",
        "good_subjects.sort()\n",
        "\n",
        "print(f\"üìã Subject List Verification:\")\n",
        "print(f\"   - Total subjects in file: {len(meta_df)}\")\n",
        "print(f\"   - Subjects marked 'Good': {len(good_subjects)}\")\n",
        "print(f\"   - First subject: {good_subjects[0]}\")\n",
        "print(f\"   - Last subject:  {good_subjects[-1]}\")\n",
        "\n",
        "if len(good_subjects) == 45:\n",
        "    print(\"\\n‚úÖ Verification PASSED: Ready to process 45 subjects.\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è WARNING: Found {len(good_subjects)} subjects instead of 45. Check your TSV file.\")"
      ],
      "metadata": {
        "id": "Njrp-_xsNjfY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPROCESSING & MACHINE LEARNER .FIF FILE EXPORTER\n",
        "success_count = 0\n",
        "\n",
        "print(f\"üöÄ STARTING EXPORT: .FIF FILES FOR MACHINE LEARNING\")\n",
        "print(f\"üìÇ Saving to: {save_path}\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "for sub_id in good_subjects:\n",
        "    print(f\"\\nProcessing: {sub_id} ...\")\n",
        "\n",
        "    try:\n",
        "        # 1. LOCATE FILE\n",
        "        sub_folder = os.path.join(base_path, sub_id)\n",
        "        set_files = glob.glob(os.path.join(sub_folder, '**', '*.set'), recursive=True)\n",
        "\n",
        "        if not set_files:\n",
        "            print(f\"   ‚ùå Skipped: No .set file found.\")\n",
        "            continue\n",
        "\n",
        "        # 2. LOAD & PREPROCESS\n",
        "        # preload=True is MANDATORY for saving .fif\n",
        "        epochs = mne.read_epochs_eeglab(set_files[0], verbose='error')\n",
        "        epochs.filter(l_freq, h_freq, verbose='error')\n",
        "        epochs.apply_baseline((-0.1, 0), verbose='error')\n",
        "\n",
        "        # 3. FILTER CONDITIONS (Control vs Violation)\n",
        "        # We need to map the IDs again to filter out irrelevant trials\n",
        "        id_to_text = {v: k for k, v in epochs.event_id.items()}\n",
        "\n",
        "        def solve_label(event_id):\n",
        "            text_label = id_to_text.get(event_id, \"\")\n",
        "            if '(110)' in text_label or '(115)' in text_label: return 'Control'\n",
        "            if '(220)' in text_label or '(225)' in text_label: return 'Violation'\n",
        "            return None\n",
        "\n",
        "        # Select only the relevant events (Control or Violation)\n",
        "        events = epochs.events\n",
        "        keep_indices = []\n",
        "        for i in range(len(events)):\n",
        "            if solve_label(events[i, -1]) in ['Control', 'Violation']:\n",
        "                keep_indices.append(i)\n",
        "\n",
        "        if len(keep_indices) == 0:\n",
        "            print(f\"   ‚ö†Ô∏è No Control/Violation trials found. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Create the final cleaned object\n",
        "        epochs_final = epochs[keep_indices]\n",
        "\n",
        "        # 4. SAVE AS .FIF\n",
        "        # This is the standard format MNE and Python ML tools expect\n",
        "        fif_name = os.path.join(save_path, f\"{sub_id}_cleaned_epo.fif\")\n",
        "        epochs_final.save(fif_name, overwrite=True, verbose='error')\n",
        "\n",
        "        print(f\"   üíæ SAVED: {sub_id}_cleaned_epo.fif\")\n",
        "        success_count += 1\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ‚ùå ERROR on {sub_id}: {str(e)}\")\n",
        "        continue\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"üéâ EXPORT COMPLETE: {success_count} .fif files saved.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Xu-9_0lENsmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VERIFICATION OF .FIF FILE EXISTENCE\n",
        "\n",
        "# Check the output folder content\n",
        "files_in_drive = os.listdir(save_path)\n",
        "fif_files = [f for f in files_in_drive if f.endswith('.fif')]\n",
        "fif_files.sort()\n",
        "\n",
        "print(f\"üìÇ VERIFICATION OF FOLDER: {save_path}\")\n",
        "print(f\"‚úÖ Found {len(fif_files)} .fif files.\")\n",
        "\n",
        "if len(fif_files) > 0:\n",
        "    print(\"\\nSample List:\")\n",
        "    print(fif_files[:45]) # Show first 5\n",
        "else:\n",
        "    print(\"‚ùå No .fif files found. Something went wrong.\")"
      ],
      "metadata": {
        "id": "ovm6oCbHPRZp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5. VISUALIZATIONS OF A GRAND AVERAGE P600 COMPONENT"
      ],
      "metadata": {
        "id": "FacFcWnhRaVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Visualization Strategy: \"Grand Average\"\n",
        "To create a clean graph for 45 participants, we cannot just plot all 2,000+ trials. We use a Grand Average approach:\n",
        "\n",
        "Level 1 (Subject): Average all \"Control\" trials for Subject 1 into a single wavy line. Do the same for \"Violation\".\n",
        "\n",
        "Level 2 (Group): Take the 45 \"Control\" lines and average them into one Grand Mean Control. Do the same for \"Violation\".\n",
        "\n",
        "Plot: Overlay the two Grand Means (with confidence intervals) to show the difference."
      ],
      "metadata": {
        "id": "oEceFcqcRiO-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PLOTTING THE GRAND AVERAGE (EEG BRAIN WAVES)"
      ],
      "metadata": {
        "id": "cmdTgWWbX50q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup & Configuration\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "base_path = '/content/drive/MyDrive/EEG_data/full_subject_list'\n",
        "results_path = '/content/drive/MyDrive/EEG_data/P600_Results/all_subs'\n",
        "tsv_path = os.path.join(base_path, 'participants.tsv')"
      ],
      "metadata": {
        "id": "KxKgZ4lNRd43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define ROI (Region of Interest)\n",
        "# These are the electrodes where the P600 component is strongest\n",
        "roi = ['Pz', 'Cz', 'CP1', 'CP2', 'P3', 'P4']\n",
        "\n",
        "# 3. Load Participant List\n",
        "if os.path.exists(tsv_path):\n",
        "    meta_df = pd.read_csv(tsv_path, sep='\\t')\n",
        "    # Filter for 'good' subjects only\n",
        "    good_subjects = meta_df[meta_df['data_usability'] == 'good']['participant_id'].tolist()\n",
        "    good_subjects.sort()\n",
        "    print(f\"‚úÖ Setup Complete. Found {len(good_subjects)} good subjects.\")\n",
        "else:\n",
        "    print(\"‚ùå Error: participants.tsv not found.\")"
      ],
      "metadata": {
        "id": "E3n3h0JrSnbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Data\n",
        "\n",
        "print(f\"üìä Loading data for {len(good_subjects)} subjects...\")\n",
        "\n",
        "# Initialize lists to hold the average brainwaves for each person\n",
        "all_evoked_control = []\n",
        "all_evoked_violation = []\n",
        "\n",
        "for sub_id in good_subjects:\n",
        "    try:\n",
        "        # Construct path to the .fif file\n",
        "        fname = os.path.join(results_path, f\"{sub_id}_cleaned_epo.fif\")\n",
        "\n",
        "        if not os.path.exists(fname):\n",
        "            print(f\"‚ö†Ô∏è Missing file for {sub_id}, skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Load the epochs (verbose=False keeps the output clean)\n",
        "        epochs = mne.read_epochs(fname, verbose=False)\n",
        "\n",
        "        # --- Identify Conditions ---\n",
        "        # We search the event IDs for our Control/Violation markers\n",
        "        ctrl_ids = [k for k in epochs.event_id if '(110)' in k or '(115)' in k]\n",
        "        viol_ids = [k for k in epochs.event_id if '(220)' in k or '(225)' in k]\n",
        "\n",
        "        # --- Average & Store ---\n",
        "        # Create a single \"Evoked\" object (average) for this subject's Control trials\n",
        "        if ctrl_ids:\n",
        "            all_evoked_control.append(epochs[ctrl_ids].average())\n",
        "\n",
        "        # Create a single \"Evoked\" object for this subject's Violation trials\n",
        "        if viol_ids:\n",
        "            all_evoked_violation.append(epochs[viol_ids].average())\n",
        "\n",
        "        print(f\".\", end=\"\") # Print a dot for progress\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error on {sub_id}: {e}\")\n",
        "\n",
        "print(f\"\\n‚úÖ DONE: Loaded {len(all_evoked_control)} Control and {len(all_evoked_violation)} Violation averages.\")"
      ],
      "metadata": {
        "id": "g4oot8HcTA84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate & Save the Plot\n",
        "\n",
        "evokeds_dict = {\n",
        "    'Control': all_evoked_control,\n",
        "    'Violation': all_evoked_violation\n",
        "}\n",
        "\n",
        "# 1. Generate the Plot\n",
        "# 'combine=\"mean\"' averages all ROI channels into one single line\n",
        "figs = mne.viz.plot_compare_evokeds(\n",
        "    evokeds_dict,\n",
        "    picks=roi,\n",
        "    combine='mean',\n",
        "    colors={'Control': 'blue', 'Violation': 'red'},\n",
        "    linestyles={'Control': '-', 'Violation': '-'},\n",
        "    ci=0.95,              # 95% Confidence Interval (Shaded area)\n",
        "    title=f'Grand Average P600 (N={len(all_evoked_control)})\\nROI: {roi}',\n",
        "    show_sensors=False,\n",
        "    legend='upper left',\n",
        "    time_unit='s'\n",
        ")\n",
        "\n",
        "# 2. Add the P600 Highlight (Grey Box)\n",
        "# We iterate through the generated figures/axes to add the shading\n",
        "for f in figs:\n",
        "    for ax in f.axes:\n",
        "        # Shade the area between 0.5s (500ms) and 0.8s (800ms)\n",
        "        ax.axvspan(0.5, 0.8, color='gray', alpha=0.2, label='P600 Window')\n",
        "        ax.legend(loc='upper right')\n",
        "\n",
        "# 3. Save to Drive\n",
        "'''save_file = os.path.join(results_path, 'Grand_Average_P600_Plot.png')\n",
        "plt.savefig(save_file, dpi=300)\n",
        "print(f\"üñºÔ∏è Plot saved to: {save_file}\")'''\n",
        "\n",
        "# 4. Show\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SoQ8xVOlTesU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Check if we have data\n",
        "if not all_evoked_control or not all_evoked_violation:\n",
        "    print(\"‚ùå Error: Data lists are empty. Please re-run Cell 2 to load the data first.\")\n",
        "else:\n",
        "    print(f\"üìâ Generating plot for {len(all_evoked_control)} subjects...\")\n",
        "\n",
        "    # 2. Generate the Plot (IMPORTANT: show=False)\n",
        "    # We set show=False so we can add the grey box BEFORE it renders\n",
        "    figs = mne.viz.plot_compare_evokeds(\n",
        "        evokeds_dict,\n",
        "        picks=roi,\n",
        "        combine='mean',\n",
        "        colors={'Control': 'blue', 'Violation': 'red'},\n",
        "        linestyles={'Control': '-', 'Violation': '-'},\n",
        "        ci=0.95,\n",
        "        title=f'Grand Average P600 (N={len(good_subjects)})\\nROI: {roi}',\n",
        "        show_sensors=False,\n",
        "        legend='upper left',\n",
        "        time_unit='s',\n",
        "        show=False  # <--- CRITICAL FIX: Don't show it yet!\n",
        "    )\n",
        "\n",
        "    # 3. Add the P600 Highlight (Grey Box)\n",
        "    # Now we can safely draw on the hidden figure\n",
        "    for i, f in enumerate(figs):\n",
        "        for ax in f.axes:\n",
        "            # Draw the P600 window (0.5 to 0.8 seconds)\n",
        "            ax.axvspan(0.5, 0.8, color='gray', alpha=0.2, label='P600 Window')\n",
        "\n",
        "            # Force the legend to update\n",
        "            ax.legend(loc='upper left')\n",
        "\n",
        "    # 4. Save and Show\n",
        "    save_file = os.path.join(results_path, 'Grand_Average_P600_Plot.png')\n",
        "    plt.savefig(save_file, dpi=300)\n",
        "    print(f\"üñºÔ∏è Plot saved to: {save_file}\")\n",
        "\n",
        "    # Now we show the final result\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "4irn3ZDJVobK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PLOTTING THE TOPOGRAPHIC MAPS"
      ],
      "metadata": {
        "id": "JhBWJYmXX_Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute Grand Averages\n",
        "# This creates a single 'Evoked' object that represents the group mean\n",
        "print(\"üß† Computing Grand Averages...\")\n",
        "\n",
        "ga_control = mne.grand_average(all_evoked_control)\n",
        "ga_violation = mne.grand_average(all_evoked_violation)\n",
        "\n",
        "print(\"‚úÖ Grand Averages Ready.\")"
      ],
      "metadata": {
        "id": "Kf1oEIEIYCJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GENERATING THE MAPS\n",
        "\n",
        "# Define the time points\n",
        "times_to_plot = [0.5, 0.6, 0.7, 0.8]  # 500ms, 600ms, 700ms, 800ms\n",
        "\n",
        "# --- PLOT 1: CONTROL CONDITION ---\n",
        "print(\"Generating Control Map...\")\n",
        "fig_ctrl = ga_control.plot_topomap(\n",
        "    times=times_to_plot,\n",
        "    ch_type='eeg',\n",
        "    average=0.05,        # Smooth over 50ms window\n",
        "    show=False,          # Keep in memory to save\n",
        "    colorbar=True,\n",
        "    vlim=(0, 5)          # FIXED SCALE: 0 to 5 ¬µV\n",
        ")\n",
        "\n",
        "# ADD TITLE MANUALLY\n",
        "fig_ctrl.suptitle('Control Condition (Grand Average)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Save\n",
        "save_path_ctrl = os.path.join(results_path, 'Topomap_Control.png')\n",
        "fig_ctrl.savefig(save_path_ctrl, dpi=300)\n",
        "print(f\"üñºÔ∏è Control Maps saved to: {save_path_ctrl}\")\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- PLOT 2: VIOLATION CONDITION ---\n",
        "print(\"Generating Violation Map...\")\n",
        "fig_viol = ga_violation.plot_topomap(\n",
        "    times=times_to_plot,\n",
        "    ch_type='eeg',\n",
        "    average=0.05,\n",
        "    show=False,\n",
        "    colorbar=True,\n",
        "    vlim=(0, 5)          # SAME SCALE as Control for fair comparison!\n",
        ")\n",
        "\n",
        "# ADD TITLE MANUALLY\n",
        "fig_viol.suptitle('Violation Condition (Grand Average)', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Save\n",
        "save_path_viol = os.path.join(results_path, 'Topomap_Violation.png')\n",
        "fig_viol.savefig(save_path_viol, dpi=300)\n",
        "print(f\"üñºÔ∏è Violation Maps saved to: {save_path_viol}\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2AAV4UEZZn6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GENERATING A SENSORY LAYOUT MAP (TOPOGRAPHY)"
      ],
      "metadata": {
        "id": "uwJw27E3b8lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Configuration\n",
        "base_path = '/content/drive/MyDrive/EEG_data/full_subject_list'\n",
        "results_path = '/content/drive/MyDrive/EEG_data/P600_Results/all_subs'\n",
        "tsv_path = os.path.join(base_path, 'participants.tsv')"
      ],
      "metadata": {
        "id": "WNBYdcBGcTRR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Get the first \"Good\" Subject\n",
        "meta_df = pd.read_csv(tsv_path, sep='\\t')\n",
        "good_subjects = meta_df[meta_df['data_usability'] == 'good']['participant_id'].tolist()\n",
        "good_subjects.sort()"
      ],
      "metadata": {
        "id": "7i7ETCKWb7VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sub = good_subjects[0] # We only need one person\n",
        "fname = os.path.join(results_path, f\"{test_sub}_cleaned_epo.fif\")\n",
        "\n",
        "print(f\"üîé Extracting channel info from: {test_sub}\")"
      ],
      "metadata": {
        "id": "fRs8x3mAcZme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(fname):\n",
        "    # Load just the header info (fast)\n",
        "    info = mne.read_epochs(fname, verbose=False).info\n",
        "\n",
        "    # --- PART 1: PRINT LIST OF CHANNELS ---\n",
        "    ch_names = info['ch_names']\n",
        "    print(f\"\\n‚úÖ Found {len(ch_names)} Channels:\")\n",
        "    print(ch_names)\n",
        "\n",
        "    # --- PART 2: PLOT SENSOR POSITIONS ---\n",
        "    # Create the figure\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "\n",
        "    # Plot 2D sensor map (Bird's eye view)\n",
        "    mne.viz.plot_sensors(\n",
        "        info,\n",
        "        kind='topomap',   # 'topomap' = flat 2D view\n",
        "        show_names=True,  # Show labels (Fz, Cz, etc.)\n",
        "        axes=ax,\n",
        "        title='EEG Sensor Layout (All Channels)',\n",
        "        show=False\n",
        "    )\n",
        "\n",
        "    # Save it\n",
        "    save_file = os.path.join(results_path, 'Sensor_Layout_Map.png')\n",
        "    plt.savefig(save_file, dpi=300)\n",
        "    print(f\"\\nüñºÔ∏è Sensor Map saved to: {save_file}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(f\"‚ùå Could not find file for {test_sub}. Please check your P600_Results folder.\")"
      ],
      "metadata": {
        "id": "t0lBZZ9McdKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 6. FILES STRUCTURE\n",
        "\n"
      ],
      "metadata": {
        "id": "mBZd79V2bavj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FILE ORGANIZATION\n",
        "\n",
        "import os\n",
        "import shutil"
      ],
      "metadata": {
        "id": "F1-iyxXPdkxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The folder to organize\n",
        "source_folder = '/content/drive/MyDrive/EEG_data/P600_Results/all_subs'\n",
        "\n",
        "folders = {\n",
        "    'png': os.path.join(source_folder, 'EEG Visualizations'),\n",
        "    'csv': os.path.join(source_folder, 'Meta Data'),\n",
        "    'fif': os.path.join(source_folder, 'Epochs with Ctrl_Viol')\n",
        "}"
      ],
      "metadata": {
        "id": "rzxn5RdqekZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(source_folder):\n",
        "    print(f\"‚ùå Error: The folder '{source_folder}' does not exist.\")\n",
        "else:\n",
        "    print(f\"üìÇ Organizing folder: {source_folder}\")\n",
        "\n",
        "    # A. Create the 3 new subfolders\n",
        "    for key, path in folders.items():\n",
        "        if not os.path.exists(path):\n",
        "            os.makedirs(path)\n",
        "            print(f\"   ‚ú® Created folder: {os.path.basename(path)}\")\n",
        "        else:\n",
        "            print(f\"   ‚úÖ Folder exists: {os.path.basename(path)}\")\n",
        "\n",
        "    # B. Move Files\n",
        "    # Get list of files (ignoring directories)\n",
        "    files = [f for f in os.listdir(source_folder) if os.path.isfile(os.path.join(source_folder, f))]\n",
        "\n",
        "    moved_counts = {'png': 0, 'csv': 0, 'fif': 0}\n",
        "\n",
        "    for filename in files:\n",
        "        src_path = os.path.join(source_folder, filename)\n",
        "\n",
        "        # Determine destination based on extension\n",
        "        if filename.endswith('.png'):\n",
        "            dst_path = os.path.join(folders['png'], filename)\n",
        "            shutil.move(src_path, dst_path)\n",
        "            moved_counts['png'] += 1\n",
        "\n",
        "        elif filename.endswith('.csv'):\n",
        "            dst_path = os.path.join(folders['csv'], filename)\n",
        "            shutil.move(src_path, dst_path)\n",
        "            moved_counts['csv'] += 1\n",
        "\n",
        "        elif filename.endswith('.fif'):\n",
        "            dst_path = os.path.join(folders['fif'], filename)\n",
        "            shutil.move(src_path, dst_path)\n",
        "            moved_counts['fif'] += 1"
      ],
      "metadata": {
        "id": "Mfb1_GFlfrNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"üéâ CLEANUP COMPLETE!\")\n",
        "print(f\"üñºÔ∏è Moved {moved_counts['png']} Images -> /EEG Visualizations\")\n",
        "print(f\"üìä Moved {moved_counts['csv']} CSVs   -> /Meta Data\")\n",
        "print(f\"üß† Moved {moved_counts['fif']} Epochs -> /Epochs with Ctrl_Viol\")\n",
        "print(\"=\"*30)"
      ],
      "metadata": {
        "id": "dpm0-zQigjqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 7. ARTIFACT REJECTION (Amplitude Thresholding)\n",
        "\n",
        "\"Artifacts were handled using an automated peak-to-peak amplitude rejection procedure. Any trial containing voltage fluctuations exceeding ¬±150 ¬µV was automatically excluded from the analysis to ensure that ocular artifacts (blinks) and muscle noise did not contaminate the P600 component. This resulted in an average retention of 93.7% of trials per participant.\""
      ],
      "metadata": {
        "id": "Z-gpBgmqhhuy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_folder = '/content/drive/MyDrive/EEG_data/P600_Results/all_subs/epochs_fifs'"
      ],
      "metadata": {
        "id": "Q1RaXMRFiHLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rejection Criteria (The \"Sneeze Filter\")\n",
        "# 150 ¬µV is a safe, standard threshold for ERPs.\n",
        "# If the difference between Min and Max voltage in a trial > 150¬µV, it's likely noise.\n",
        "reject_criteria = dict(eeg=150e-6)  # 150 ¬µV"
      ],
      "metadata": {
        "id": "QkoGl2LviVsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fif_files = glob.glob(os.path.join(epochs_folder, '*.fif'))\n",
        "fif_files.sort()\n",
        "\n",
        "report_data = []\n",
        "\n",
        "for fname in fif_files:\n",
        "    sub_id = os.path.basename(fname).split('_')[0]\n",
        "\n",
        "    try:\n",
        "        # 1. Load Data\n",
        "        epochs = mne.read_epochs(fname, preload=True, verbose=False)\n",
        "        original_count = len(epochs)\n",
        "\n",
        "        # 2. Drop Bad Epochs\n",
        "        # This function automatically finds and drops trials exceeding the threshold\n",
        "        epochs.drop_bad(reject=reject_criteria, verbose=False)\n",
        "\n",
        "        final_count = len(epochs)\n",
        "        dropped_count = original_count - final_count\n",
        "        percent_lost = (dropped_count / original_count) * 100\n",
        "\n",
        "        # 3. Save OVER the old file (Update it with the clean version)\n",
        "        # We overwrite because we don't want dirty data for ML\n",
        "        epochs.save(fname, overwrite=True, verbose=False)\n",
        "\n",
        "        # 4. Log Stats\n",
        "        report_data.append({\n",
        "            'Subject': sub_id,\n",
        "            'Original_Trials': original_count,\n",
        "            'Kept_Trials': final_count,\n",
        "            'Dropped_Trials': dropped_count,\n",
        "            'Percent_Lost': round(percent_lost, 1)\n",
        "        })\n",
        "\n",
        "        # Print status\n",
        "        if percent_lost > 25:\n",
        "            print(f\"‚ö†Ô∏è {sub_id}: High data loss! Dropped {dropped_count} trials ({percent_lost}%).\")\n",
        "        else:\n",
        "            print(f\"‚úÖ {sub_id}: Cleaned. Dropped {dropped_count} trials.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error on {sub_id}: {e}\")"
      ],
      "metadata": {
        "id": "ZzNWZ_CiiZFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# SUMMARY REPORT\n",
        "# ==========================================\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"üèÅ HYGIENE CHECK COMPLETE\")\n",
        "\n",
        "# Show who had the worst data\n",
        "df_report = pd.DataFrame(report_data)\n",
        "print(\"\\nüìä Data Quality Summary:\")\n",
        "print(f\"   - Average trials kept per subject: {int(df_report['Kept_Trials'].mean())}\")\n",
        "print(f\"   - Average data loss: {round(df_report['Percent_Lost'].mean(), 1)}%\")\n",
        "\n",
        "# Check for subjects with too few trials (e.g., < 20)\n",
        "bad_subjects = df_report[df_report['Kept_Trials'] < 20]\n",
        "if not bad_subjects.empty:\n",
        "    print(f\"\\n‚ö†Ô∏è WARNING: The following subjects have very little data left (<20 trials):\")\n",
        "    print(bad_subjects[['Subject', 'Kept_Trials']])\n",
        "    print(\"Consider excluding them from your final analysis.\")\n",
        "else:\n",
        "    print(\"\\n‚ú® All subjects have sufficient data remaining.\")"
      ],
      "metadata": {
        "id": "b4rmgB9Vit1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TO DO: DROP SUB-059 from CSV and run VISUALIZATIONS AGAIN! (.fifs have alreayd been updated)**\n",
        "\n",
        "Due to high data loss (41 trials dropped / 51.2%). Sub-059's data is too noisy"
      ],
      "metadata": {
        "id": "sGm_ul3fj1Tz"
      }
    }
  ]
}